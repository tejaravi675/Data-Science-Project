# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pdFuiRz6cDwn_jd2nFdUD06iL4Y_A8Ne
"""

pip install vecstack

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
import xgboost as xgb
from vecstack import stacking
from sklearn.model_selection import cross_validate, GridSearchCV, train_test_split, KFold, RepeatedKFold
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score
from keras.models import Sequential
from keras.layers import Dense
import tensorflow as tf

features = pd.read_csv(r"/content/drive/MyDrive/DS Project/processed features - Complete.csv", delimiter=";" ,names=["Label", "Number of Samples", "Min", "Max", "Median", "Mean", "SD"])

print(features.head())
print(features.info())

features["Label"] = features["Label"].astype(int)
pd.options.display.float_format = "{:,.2f}".format
print(features.info())
print(features.head())

features["Label"].value_counts()

cols = ["Min", "Max", "Median", "Mean", "SD"]
X = features[cols]
y = features["Label"]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=None)
print("x train: ",X_train.shape)
print("x test: ",X_test.shape)
print("y train: ",y_train.shape)
print("y test: ",y_test.shape)

print(X.shape)
print(y.shape)

"""Models

1. Logistic Regression using cross validation
"""

#with hyperparameter tuning
LR = LogisticRegression()

#Search grid for optimal parameters
lr_param_grid = {"penalty": ["l1", "l2", "elasticnet", "none"],
                 "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"],
                 "C": [0.0001, 0.001, 0.01, 0.1, 0.3, 0.5, 1.0]}

gsLR = GridSearchCV(LR, param_grid = lr_param_grid, cv=10, scoring='accuracy', n_jobs= -1, verbose = 1)

gsLR.fit(X,y)

lr_best = gsLR.best_estimator_

print(lr_best)
print(gsLR.best_score_)

scoring = {'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'auc': 'roc_auc'}

modelCV = LogisticRegression(C= 1.0, penalty='l1', solver='liblinear')

results = cross_validate(modelCV, X, y, cv=5, scoring=list(scoring.values()), 
                         return_train_score=False)

print('K-fold cross-validation results:')
for sc in range(len(scoring)):
    print(modelCV.__class__.__name__+" %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], results['test_%s' % list(scoring.values())[sc]].mean(), 
                                                              results['test_%s' % list(scoring.values())[sc]].std()))

# y_pred = gsLR.predict(X_test)

# print("\n GridSearch Results:")
# print("accuracy is %2.3f" % accuracy_score(y_test, y_pred))
# print("precision is %2.3f" % precision_score(y_test, y_pred))
# print("recall is %2.3f" % recall_score(y_test, y_pred))

"""2. Decision Trees with AdaBoost"""

#with hyperparameter tuning
DTC = DecisionTreeClassifier()

adaDTC = AdaBoostClassifier(DTC, random_state=7)

#Search grid for optimal parameters
ada_param_grid = {"base_estimator__criterion" : ["gini", "entropy"],
              "base_estimator__splitter" :   ["best", "random"],
              "algorithm" : ["SAMME","SAMME.R"],
              "n_estimators" :[10,20,30,40,50],
              "learning_rate":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3, 1.5]}

gsadaDTC = GridSearchCV(adaDTC, param_grid = ada_param_grid, cv=10, scoring='accuracy', n_jobs= -1, verbose = 1)

gsadaDTC.fit(X,y)

ada_best = gsadaDTC.best_estimator_

print(ada_best)
print(gsadaDTC.best_score_)

scoring = {'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'auc': 'roc_auc'}

modelCV = AdaBoostClassifier(DecisionTreeClassifier(criterion="entropy", splitter='random'))

results = cross_validate(modelCV, X, y, cv=5, scoring=list(scoring.values()), 
                         return_train_score=False)

print('K-fold cross-validation results:')
for sc in range(len(scoring)):
    print(modelCV.__class__.__name__+" %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], results['test_%s' % list(scoring.values())[sc]].mean(), 
                                                              results['test_%s' % list(scoring.values())[sc]].std()))
    
# y_pred = gsadaDTC.predict(X_test)

# print("accuracy is %2.3f" % accuracy_score(y_test, y_pred))
# print("precision is %2.3f" % precision_score(y_test, y_pred))
# print("recall is %2.3f" % recall_score(y_test, y_pred))

"""3. Random Forest"""

#with hyperparameter tuning
RFC = RandomForestClassifier()

#Search grid for optimal parameters
rf_param_grid = {"max_features": [1, 3, 10],
              "min_samples_split": [2, 3, 10],
              "min_samples_leaf": [1, 3, 10],
              "n_estimators" :[100,200,300],
              "criterion": ["gini", "entropy"]}

gsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=5, scoring="accuracy", n_jobs= -1, verbose = 1)

gsRFC.fit(X,y)

rfc_best = gsRFC.best_estimator_

print(rfc_best)
print(gsRFC.best_score_)

scoring = {'accuracy': 'accuracy', 'precision': 'precision', 'recall': 'recall', 'auc': 'roc_auc'}

modelCV = RandomForestClassifier(criterion="gini", min_samples_split=2, min_samples_leaf=10, max_features=3, n_estimators=10)

results = cross_validate(modelCV, X, y, cv=5, scoring=list(scoring.values()), 
                         return_train_score=False)

print('K-fold cross-validation results:')
for sc in range(len(scoring)):
    print(modelCV.__class__.__name__+" %s: %.3f (+/-%.3f)" % (list(scoring.keys())[sc], results['test_%s' % list(scoring.values())[sc]].mean(), 
                                                              results['test_%s' % list(scoring.values())[sc]].std()))

# y_pred = gsRFC.predict(X_test)

# print("accuracy is %2.3f" % accuracy_score(y_test, y_pred))
# print("precision is %2.3f" % precision_score(y_test, y_pred))
# print("recall is %2.3f" % recall_score(y_test, y_pred))

"""4. Artificial Neural Network"""

#defining the model
model = Sequential()
model.add(Dense(12, input_dim=5, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# compile the keras model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])

# fit the keras model on the dataset
model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, batch_size=128)

# evaluate the keras model
_, train_acc, train_pre, train_rec, train_auc = model.evaluate(X_train, y_train, verbose=0)
_, test_acc, test_pre, test_rec, test_auc = model.evaluate(X_test, y_test, verbose=0)
print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))
print('Train: %.3f, Test: %.3f' % (train_pre, test_pre))
print('Train: %.3f, Test: %.3f' % (train_rec, test_rec))
print('Train: %.3f, Test: %.3f' % (train_auc, test_auc))

"""5. Stacking

Using Alogrithms like DT, RF, Gradient Boosting, Extra Trees Classifier
"""

# Put in our parameters for said classifiers
# Extra Trees Parameters
et_params = {
    'n_jobs': -1,
    'max_depth': 8,
    'min_samples_leaf': 2,
    'verbose': 0
}

# Gradient Boosting parameters
gb_params = {
    'n_estimators': 500,
    'max_depth': 5,
    'min_samples_leaf': 2,
    'verbose': 0
}

def param(clf, param):
  return clf(**param)

# Create 5 objects that represent our 4 models
lr = LogisticRegression(C= 1.0, penalty='l2')
rf = RandomForestClassifier(criterion="gini", min_samples_split=2, min_samples_leaf=10, max_features=3, n_estimators=10)
et = param(ExtraTreesClassifier, et_params)
ada = AdaBoostClassifier(DecisionTreeClassifier(criterion="entropy", splitter='random'))
gb = param(GradientBoostingClassifier, gb_params)

#combine all into one model
models = [lr, rf, et, ada, gb]

# Create our OOF train and test predictions
L1_train, L1_test = stacking(models, X_train, y_train, X_test, regression=False, n_folds=5)

print(L1_train.shape)
print(L1_test.shape)

"""Using XGBoostClassifier as meta_model"""

gbm = xgb.XGBClassifier(
 n_estimators= 2000,
 max_depth= 4,
 min_child_weight= 2,
 gamma=0.9,                        
 subsample=0.8,
 colsample_bytree=0.8,
 objective= 'binary:logistic',
 nthread= -1,
 scale_pos_weight=1)

gbm.fit(L1_train, y_train)
y_pred = gbm.predict(L1_test)

score = accuracy_score(y_test, y_pred)
score1 = precision_score(y_test, y_pred)
score2 = recall_score(y_test, y_pred)
score3 = roc_auc_score(y_test, y_pred)
print(score)
print(score1)
print(score2)
print(score3)